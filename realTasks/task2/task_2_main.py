import json
import re

def normalize_word(word):
    """
    —É–¥–∞–ª—è–µ—Ç –≤—Å–µ –Ω–µ–±—É–∫–≤—ã–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ -
    """
    new_word = ''

    for symbol in word:
        if (symbol.isalpha() or symbol == '-'):
            new_word += symbol
    
    return new_word.lower()

def get_text_from_message(message):
    if isinstance(message['text'], str):
        return message['text']
    else:
        text = ''
        for token in message['text']:
            if isinstance(token, str):
                text += token
        
        return text


def get_urls_from_message(message):
    if not isinstance(message['text'], list):
        return []
    else:
        links = []
        for token in message['text']:
            if isinstance(token, dict):
                if token['type'] == 'link':
                    links.append(token['text'])
                if token['type'] == 'text_link':
                    links.append(token['href'])
        
        return links

def get_mentions_from_message(message):
    if not isinstance(message['text'], list):
        return []
    else:
        mentions = []
        for token in message['text']:
            if isinstance(token, dict):
                if token['type'] == 'mention':
                    mentions.append(token['text'])
        
        return mentions


def get_FQDN_from_url(url):
    return re.match(r'^(http)?s?(:\/\/)?(www.)?([\w\.]+).*$', url).group(4)

if __name__ == "__main__":
    json_file_name = 'result.json'

    chat = json.load(open(json_file_name))

    professors = [
        'Anna Klezovich', 'Nikita Sapunov', 'Oleg Serikov', 'Olga ùì∏ùìµùìÆùìºùì™ùìª Lyashevskaya', 
        '@annaklezovich', '@sapunov', '@oserikov', '@olesar'
        ]
    assistants = [
        'Katya Taktasheva', 'Olga Pitchuzhkina', 'Anton Buzanov', 'Alina Rogulina',
        '@tak_ty', '@olgap981', '@Vantral', '@avrogulina'
    ]


    professors_phrases = {}
    assistants_phrases = {}
    student_phrases = {}

    unique_FQDNs = set()
    
    result_mentions = {'student': 0, 'other': 0}
    for message in chat['messages']:
        if 'from' not in message:
            # not text message
            continue

        name = message['from']
        
        text = get_text_from_message(message)
        
        words = text.split()
        for i in range(len(words)):
            words[i] = normalize_word(words[i])  
        
        # –ø–æ–¥–∑–∞–¥–∞—á–∏ 2-4
        for word in words:
            if word == '':
                continue
            if name in professors:
                # –Ω–∞—à–ª–∏ –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä
                professors_phrases.setdefault(word, 0)
                professors_phrases[word] += 1
            
            elif name in assistants:
                # –Ω–∞—à–ª–∏ –¢–ê
                assistants_phrases.setdefault(word, 0)
                assistants_phrases[word] += 1
            else:
                # –Ω–∞—à–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞
                student_phrases.setdefault(word, 0)
                student_phrases[word] += 1
        
        # –ø–æ–¥–∑–∞–¥–∞—á–∞ 5
        links = get_urls_from_message(message)
        FQDNs = set()
        for link in links:
            FQDNs.add(get_FQDN_from_url(link))
        unique_FQDNs = unique_FQDNs | FQDNs

        # –ø–æ–¥–∑–∞–¥–∞—á–∞ 6
        mentions = get_mentions_from_message(message)
        for mention in mentions:
            if mention in professors or mention in assistants:
                # –µ—Å–ª–∏ –ø–∏–Ω–≥ –Ω–∞ –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∏–ª–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                result_mentions['other'] += 1
            else:
                # –µ—Å–ª–∏ –ø–∏–Ω–≥ –Ω–∞ —Å—Ç—É–¥–µ–Ω—Ç–∞
                result_mentions['student'] += 1

    def _max_prof(key):
        return -professors_phrases[key]

    print('–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞:')
    for word in sorted(professors_phrases, key=_max_prof)[:10]:
        print(word, professors_phrases[word])
    print()

    def _max_assis(key):
        return -assistants_phrases[key]

    print('–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã:')
    for word in sorted(assistants_phrases, key=_max_assis)[:10]:
        print(word, assistants_phrases[word])
    print()

    def _max_stud(key):
        return -student_phrases[key]

    print('–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã:')
    for word in sorted(student_phrases, key=_max_stud)[:10]:
        print(word, student_phrases[word])
    
    print()
    print('—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏: ' + str(unique_FQDNs))

    print()
    print('mentions: ' + str(result_mentions))
    print('–°—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø–∏–Ω–≥–æ–≤–∞–ª–∏ –º–µ–Ω—å—à–µ, —á–µ–º –¥—Ä—É–≥–∏—Ö')

"""
–ò—Ç–æ–≥:

–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞:
–≤ 375
–∏ 309
–Ω–µ 272
–Ω–∞ 221
–µ—Å–ª–∏ 165
—è 164
—á—Ç–æ 154
—Ç–æ 125
–±—É–¥–µ—Ç 120
–∞ 119

–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã:
–≤ 78
–Ω–µ 68
–∑–∞ 52
–∏ 46
–æ—Ü–µ–Ω–∫–∏ 46
–¥–∑ 42
–µ—Å–ª–∏ 34
—á—Ç–æ 33
—è 29
–∞ 27

–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã:
–≤ 236
–∞ 169
–Ω–µ 118
—Å–ø–∞—Å–∏–±–æ 110
–∏ 106
—á—Ç–æ 95
–ø–æ 88
–Ω–∞ 79
–∏–ª–∏ 75
—Å 72

—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {'colab.research.google.com', 'ummerofcode.withgoogle.com', 'stackoverflow.com', 'b.py', 'pip.pypa.io', 't.me', 'youtu.be', 'padlite.spline.de', 'docs.python.org', 'ria.ru', 'youtube.com', 'github.com', 'etherpad.net', 'regular', 'summerofcode.withgoogle.com', 'kaggle.com', 'interestinglife.ru', 'pythontutor.com', 'docs.google.com', 'doodle.com', 'us04web.zoom.us', 'file.read', 'pykili.github.io', 'zoom.us', 'forms.gle', 'google.com', 'reddit.com', 'pythonworld.ru', 'a.py', 'wiki.cs.hse.ru', 'jetbrains.com', 'gist.github.com', 'drive.google.com', 'letnyayashkola.org', 'online', 'bit.ly', 'classroom.github.com', 'regex101.com', 'tproger.ru'}

{'student': 6, 'other': 57}
–°—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø–∏–Ω–≥–æ–≤–∞–ª–∏ –º–µ–Ω—å—à–µ, —á–µ–º –¥—Ä—É–≥–∏—Ö
"""